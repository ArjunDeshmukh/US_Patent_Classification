{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Importing Libraries\n","metadata":{}},{"cell_type":"code","source":"import requests\nimport json\nimport sys\nimport pandas as pd\nimport os.path\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\n\nimport time\nfrom tqdm import tqdm\n\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\n\nfrom transformers import AutoModelForMaskedLM\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, DataCollatorWithPadding\n\nimport torch\nfrom torch.profiler import profile, record_function, ProfilerActivity\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import Dataset, DataLoader, IterableDataset\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-10-01T13:54:27.252645Z","iopub.execute_input":"2022-10-01T13:54:27.253613Z","iopub.status.idle":"2022-10-01T13:54:34.383166Z","shell.execute_reply.started":"2022-10-01T13:54:27.253517Z","shell.execute_reply":"2022-10-01T13:54:34.382100Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Define configuration constants","metadata":{}},{"cell_type":"code","source":"class CFG:\n    CPC_CODES_PATH = \"../input/cpc-code-upto-subclass/CPC_codes_upto_subclass.csv\"\n    BERT_FOR_PATENTS_PATH = \"../input/bert-for-patents/bert-for-patents-pytorch\"\n    DEBERTA_V3_LARGE_PATH = \"../input/deberta-v3-large/deberta-v3-large\"\n    CPC_CODES_NUM_PATENTS_PATH = \"../input/cpc-code-num-patents/cpc_code_num_patents.csv\"\n    MAX_TOKEN_LEN = 512\n    DROPOUT_PROB = 0.2\n    ATTENTION_INTERMEDIATE_SIZE = 512\n    GOOGLE_CLOUD_CRED_JSON = '../input/googlecloudcred/us-patent-classification-d344a6ddc702.json'\n    GOOGLE_CLOUD_PROJ_ID = 'us-patent-classification'\n    BATCH_SIZE = 8\n    NUM_EPOCHS = 5\n    TRAIN_PATENT_START_YEAR = 2001\n    TRAIN_PATENT_END_YEAR = 2018\n    TEST_PATENT_START_YEAR = 2019\n    TEST_PATENT_END_YEAR = 2022\n    NUM_FOLDS = 6 #Total number of years used for training should be divisible by this number\n    ENCODER_LR = 2e-5\n    DECODER_LR = 2e-5\n    MIN_LR = 1e-6\n    EPS = 1e-6\n    WEIGHT_DECAY = 0.01\n    SCHEDULER = 'linear'\n    NUM_WARMUP_STEPS = 0\n    NUM_CYCLES = 0.5\n    BETAS=(0.9, 0.999)\n    MAX_GRAD_NORM = 1000\n    PRINT_FREQ = 5\n    INFINITY = 1e6\n    F_TRAIN = 1\n    \n    # Parameters which will be added in subsequent code:\n    # NUM_CLASSES, TRAIN_NUM_PATENTS","metadata":{"execution":{"iopub.status.busy":"2022-10-01T13:54:34.385272Z","iopub.execute_input":"2022-10-01T13:54:34.386952Z","iopub.status.idle":"2022-10-01T13:54:34.394829Z","shell.execute_reply.started":"2022-10-01T13:54:34.386912Z","shell.execute_reply":"2022-10-01T13:54:34.393868Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### List of CPC codes","metadata":{}},{"cell_type":"code","source":"cpc_codes_upto_subclass_df = pd.read_csv(CFG.CPC_CODES_PATH)\ncpc_codes_upto_subclass_df.head()\nnum_codes = len(pd.unique(cpc_codes_upto_subclass_df['code']))\ndf_len = len(cpc_codes_upto_subclass_df)\nCFG.NUM_CLASSES = df_len\nprint(\"Number of rows in dataframe: \", df_len, \"\\n\")\nprint(\"Number of CPC codes upto subclass: \", num_codes)\ncpc_code_dict = dict(zip(cpc_codes_upto_subclass_df['code'].values, cpc_codes_upto_subclass_df.index))","metadata":{"execution":{"iopub.status.busy":"2022-10-01T13:54:34.398390Z","iopub.execute_input":"2022-10-01T13:54:34.398747Z","iopub.status.idle":"2022-10-01T13:54:34.441865Z","shell.execute_reply.started":"2022-10-01T13:54:34.398719Z","shell.execute_reply":"2022-10-01T13:54:34.440885Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Number of rows in dataframe:  674 \n\nNumber of CPC codes upto subclass:  674\n","output_type":"stream"}]},{"cell_type":"code","source":"list(cpc_code_dict.keys())[:1]","metadata":{"execution":{"iopub.status.busy":"2022-10-01T13:54:34.443444Z","iopub.execute_input":"2022-10-01T13:54:34.443801Z","iopub.status.idle":"2022-10-01T13:54:34.454879Z","shell.execute_reply.started":"2022-10-01T13:54:34.443764Z","shell.execute_reply":"2022-10-01T13:54:34.452908Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"['A01B']"},"metadata":{}}]},{"cell_type":"markdown","source":"### Accessing Google Cloud BigQuery Public Patent Data","metadata":{}},{"cell_type":"code","source":"credentials = service_account.Credentials.from_service_account_file(CFG.GOOGLE_CLOUD_CRED_JSON)\n\nproject_id = CFG.GOOGLE_CLOUD_PROJ_ID\nbq_client = bigquery.Client(credentials= credentials, project = project_id)","metadata":{"execution":{"iopub.status.busy":"2022-10-01T13:54:34.458993Z","iopub.execute_input":"2022-10-01T13:54:34.459252Z","iopub.status.idle":"2022-10-01T13:54:34.473896Z","shell.execute_reply.started":"2022-10-01T13:54:34.459227Z","shell.execute_reply":"2022-10-01T13:54:34.473050Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"'''\n\nstart_time = time.time()\n    \ncpc_code = cpc_codes_upto_subclass_df.iloc[1]['code']\n\nquery1 = r\"\"\"\nSELECT DISTINCT t1.id, t2.group_id, t1.abstract\nFROM `patents-public-data.patentsview.patent` t1, `patents-public-data.patentsview.cpc_current` t2 \nWHERE t1.id = t2.patent_id AND t1.type='utility' AND timestamp(t1.date) >= timestamp('2000-01-01') AND timestamp(t1.date) <= timestamp('2018-12-31')\nORDER BY t1.id ASC, t2.group_id\nLIMIT 64\nOFFSET 0\n\"\"\"\n\nquery2 = r\"\"\"\nSELECT COUNT(CPCCode) AS num_patents\nFROM(\nSELECT DISTINCT t1.id AS PatentID, t2.group_id AS CPCCode\nFROM `patents-public-data.patentsview.patent` t1,\n`patents-public-data.patentsview.cpc_current` t2 where t1.id = t2.patent_id\nand t1.type='utility'\nand t2.group_id = '{}'\nand timestamp(t1.date) >= timestamp('2000-01-01')\n)\n\"\"\".format(cpc_code)\n\n\ndf1 = bq_client.query(query1).to_dataframe()\n#df2 = bq_client.query(query2).to_dataframe()\n\n    \nend_time = time.time()\n\nprint(\"Time taken: \", end_time - start_time, \" seconds\")\n\ndf_curr = df1.loc[df1['id'] == df1['id'].unique()[1]]\nlabel_list = []\nlabel_list.append(list(df_curr['group_id']))\nlabel_list\n\n'''","metadata":{"execution":{"iopub.status.busy":"2022-10-01T13:54:34.476625Z","iopub.execute_input":"2022-10-01T13:54:34.476912Z","iopub.status.idle":"2022-10-01T13:54:34.483640Z","shell.execute_reply.started":"2022-10-01T13:54:34.476880Z","shell.execute_reply":"2022-10-01T13:54:34.482579Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'\\n\\nstart_time = time.time()\\n    \\ncpc_code = cpc_codes_upto_subclass_df.iloc[1][\\'code\\']\\n\\nquery1 = r\"\"\"\\nSELECT DISTINCT t1.id, t2.group_id, t1.abstract\\nFROM `patents-public-data.patentsview.patent` t1, `patents-public-data.patentsview.cpc_current` t2 \\nWHERE t1.id = t2.patent_id AND t1.type=\\'utility\\' AND timestamp(t1.date) >= timestamp(\\'2000-01-01\\') AND timestamp(t1.date) <= timestamp(\\'2018-12-31\\')\\nORDER BY t1.id ASC, t2.group_id\\nLIMIT 64\\nOFFSET 0\\n\"\"\"\\n\\nquery2 = r\"\"\"\\nSELECT COUNT(CPCCode) AS num_patents\\nFROM(\\nSELECT DISTINCT t1.id AS PatentID, t2.group_id AS CPCCode\\nFROM `patents-public-data.patentsview.patent` t1,\\n`patents-public-data.patentsview.cpc_current` t2 where t1.id = t2.patent_id\\nand t1.type=\\'utility\\'\\nand t2.group_id = \\'{}\\'\\nand timestamp(t1.date) >= timestamp(\\'2000-01-01\\')\\n)\\n\"\"\".format(cpc_code)\\n\\n\\ndf1 = bq_client.query(query1).to_dataframe()\\n#df2 = bq_client.query(query2).to_dataframe()\\n\\n    \\nend_time = time.time()\\n\\nprint(\"Time taken: \", end_time - start_time, \" seconds\")\\n\\ndf_curr = df1.loc[df1[\\'id\\'] == df1[\\'id\\'].unique()[1]]\\nlabel_list = []\\nlabel_list.append(list(df_curr[\\'group_id\\']))\\nlabel_list\\n\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Count the number of patents for each cpc_code","metadata":{}},{"cell_type":"code","source":"if os.path.exists(CFG.CPC_CODES_NUM_PATENTS_PATH):\n    \n    cpc_code_number_df = pd.read_csv(CFG.CPC_CODES_NUM_PATENTS_PATH)\n    \nelse:\n\n    cpc_code_number_df = pd.DataFrame() \n\n    for cpc_code in tqdm(list(cpc_code_dict.keys())):\n        \n        start_date = r'{}-01-01'.format(CFG.TRAIN_PATENT_START_YEAR)\n        end_date = r'{}-12-31'.format(CFG.TRAIN_PATENT_END_YEAR)\n\n        query = r\"\"\"\n                 SELECT COUNT(CPCCode) AS num_patents\n                 FROM(\n                 SELECT DISTINCT t1.id AS PatentID, t2.group_id AS CPCCode\n                 FROM `patents-public-data.patentsview.patent` t1,\n                 `patents-public-data.patentsview.cpc_current` t2 where t1.id = t2.patent_id\n                 and t1.type='utility'\n                 and t2.group_id = '{}'\n                 and timestamp(t1.date) >= timestamp('{}') and timestamp(t1.date) <= timestamp('{}')\n                 )\"\"\".format(cpc_code, start_date, end_date)\n        \n        df = bq_client.query(query).to_dataframe()\n        df.insert(loc = 0, column = 'cpc_code', value = cpc_code)\n\n        cpc_code_number_df = cpc_code_number_df.append(df, ignore_index = True)\n\n\n    cpc_code_number_df.to_csv('./cpc_code_num_patents.csv')","metadata":{"execution":{"iopub.status.busy":"2022-10-01T13:54:34.485501Z","iopub.execute_input":"2022-10-01T13:54:34.486217Z","iopub.status.idle":"2022-10-01T13:54:34.506332Z","shell.execute_reply.started":"2022-10-01T13:54:34.486181Z","shell.execute_reply":"2022-10-01T13:54:34.505281Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def find_num_dstnct_patent_cpc_in_years(start_year, end_year):\n    \n    start_date = r'{}-01-01'.format(start_year)\n    end_date = r'{}-12-31'.format(end_year)\n    \n    query = r\"\"\"\n                 SELECT COUNT(CPCCode) AS dstnct_patent_cpc\n                 FROM(\n                 SELECT DISTINCT t1.id AS PatentID, t2.group_id AS CPCCode\n                 FROM `patents-public-data.patentsview.patent` t1,\n                 `patents-public-data.patentsview.cpc_current` t2 where t1.id = t2.patent_id\n                 and t1.type='utility'\n                 and timestamp(t1.date) >= timestamp('{}') and timestamp(t1.date) <= timestamp('{}')\n                 )\"\"\".format(start_date, end_date)\n    \n    df = bq_client.query(query).to_dataframe()\n    return df['dstnct_patent_cpc'][0]","metadata":{"execution":{"iopub.status.busy":"2022-10-01T13:54:34.508736Z","iopub.execute_input":"2022-10-01T13:54:34.509366Z","iopub.status.idle":"2022-10-01T13:54:34.515154Z","shell.execute_reply.started":"2022-10-01T13:54:34.509331Z","shell.execute_reply":"2022-10-01T13:54:34.514152Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"m = find_num_dstnct_patent_cpc_in_years(2001, 2018)\nm","metadata":{"execution":{"iopub.status.busy":"2022-10-01T13:54:34.517384Z","iopub.execute_input":"2022-10-01T13:54:34.517680Z","iopub.status.idle":"2022-10-01T13:54:35.726077Z","shell.execute_reply.started":"2022-10-01T13:54:34.517646Z","shell.execute_reply":"2022-10-01T13:54:35.724876Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"7970125"},"metadata":{}}]},{"cell_type":"code","source":"def find_num_patents_in_years(start_year, end_year):\n    \n    start_date = r'{}-01-01'.format(start_year)\n    end_date = r'{}-12-31'.format(end_year)\n    \n    query = r\"\"\" SELECT COUNT(PatentID) AS total_num_patents\n                 FROM(\n                 SELECT DISTINCT t1.id as PatentID\n                 FROM `patents-public-data.patentsview.patent` t1\n                 where t1.type='utility'\n                 and timestamp(t1.date) >= timestamp('{}') and timestamp(t1.date) <= timestamp('{}')\n                 )\"\"\".format(start_date, end_date)\n    \n    df = bq_client.query(query).to_dataframe()\n    return df['total_num_patents'][0]","metadata":{"execution":{"iopub.status.busy":"2022-10-01T13:54:35.727448Z","iopub.execute_input":"2022-10-01T13:54:35.727926Z","iopub.status.idle":"2022-10-01T13:54:35.734317Z","shell.execute_reply.started":"2022-10-01T13:54:35.727879Z","shell.execute_reply":"2022-10-01T13:54:35.733193Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"n = find_num_patents_in_years(2003, 2003)\nn","metadata":{"execution":{"iopub.status.busy":"2022-10-01T13:54:35.736185Z","iopub.execute_input":"2022-10-01T13:54:35.736552Z","iopub.status.idle":"2022-10-01T13:54:36.698996Z","shell.execute_reply.started":"2022-10-01T13:54:35.736509Z","shell.execute_reply":"2022-10-01T13:54:36.697866Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"169077"},"metadata":{}}]},{"cell_type":"code","source":"plt.plot(cpc_code_number_df.index, cpc_code_number_df['num_patents'])\nplt.xlabel('CPC Code Index')\nplt.ylabel('Number of Patents')\n\nprint(\"Total number of patents: \", cpc_code_number_df['num_patents'].sum())\nCFG.TRAIN_NUM_PATENTS = cpc_code_number_df['num_patents'].sum()","metadata":{"execution":{"iopub.status.busy":"2022-10-01T13:54:36.700326Z","iopub.execute_input":"2022-10-01T13:54:36.701047Z","iopub.status.idle":"2022-10-01T13:54:36.932457Z","shell.execute_reply.started":"2022-10-01T13:54:36.701007Z","shell.execute_reply":"2022-10-01T13:54:36.931502Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Total number of patents:  7970118\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"CFG.TRAIN_NUM_PATENTS","metadata":{"execution":{"iopub.status.busy":"2022-10-01T13:54:36.934195Z","iopub.execute_input":"2022-10-01T13:54:36.934578Z","iopub.status.idle":"2022-10-01T13:54:36.944399Z","shell.execute_reply.started":"2022-10-01T13:54:36.934536Z","shell.execute_reply":"2022-10-01T13:54:36.943490Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"7970118"},"metadata":{}}]},{"cell_type":"markdown","source":"### Calculate weights to handle class imbalance","metadata":{}},{"cell_type":"code","source":"cpc_code_number_df['weights'] = cpc_code_number_df['num_patents'].sum()/cpc_code_number_df['num_patents']\ncpc_code_number_df['weights'].replace(np.inf, 0.0, inplace = True)\ncpc_code_number_df['weights'] = cpc_code_number_df['weights']/cpc_code_number_df['weights'].sum()","metadata":{"execution":{"iopub.status.busy":"2022-10-01T13:54:36.950100Z","iopub.execute_input":"2022-10-01T13:54:36.950381Z","iopub.status.idle":"2022-10-01T13:54:36.959089Z","shell.execute_reply.started":"2022-10-01T13:54:36.950354Z","shell.execute_reply":"2022-10-01T13:54:36.958012Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Run a sample input through the BERT for Patents model","metadata":{}},{"cell_type":"code","source":"model_bert_for_patents = AutoModel.from_pretrained(CFG.BERT_FOR_PATENTS_PATH)\n\ntokenizer = AutoTokenizer.from_pretrained(CFG.BERT_FOR_PATENTS_PATH)\nCFG.tokenizer = tokenizer\n\nconfig = AutoConfig.from_pretrained(CFG.BERT_FOR_PATENTS_PATH)\nsent = \"This is a patent.\"\ntokenized_sent = tokenizer(sent, padding = True, truncation = True, return_tensors = \"pt\")\nprint(tokenized_sent)\nout = model_bert_for_patents(**tokenized_sent)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-01T13:54:36.960784Z","iopub.execute_input":"2022-10-01T13:54:36.961202Z","iopub.status.idle":"2022-10-01T13:54:52.684760Z","shell.execute_reply.started":"2022-10-01T13:54:36.961165Z","shell.execute_reply":"2022-10-01T13:54:52.683710Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at ../input/bert-for-patents/bert-for-patents-pytorch were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"{'input_ids': tensor([[   2, 1688, 1668, 1042, 7018, 1017,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n","output_type":"stream"}]},{"cell_type":"code","source":"config","metadata":{"execution":{"iopub.status.busy":"2022-10-01T13:54:52.686492Z","iopub.execute_input":"2022-10-01T13:54:52.688106Z","iopub.status.idle":"2022-10-01T13:54:52.696712Z","shell.execute_reply.started":"2022-10-01T13:54:52.688061Z","shell.execute_reply":"2022-10-01T13:54:52.695617Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"BertConfig {\n  \"_name_or_path\": \"../input/bert-for-patents/bert-for-patents-pytorch\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 39859\n}"},"metadata":{}}]},{"cell_type":"code","source":"print(out.last_hidden_state.shape)","metadata":{"execution":{"iopub.status.busy":"2022-10-01T13:54:52.698742Z","iopub.execute_input":"2022-10-01T13:54:52.699333Z","iopub.status.idle":"2022-10-01T13:54:52.706966Z","shell.execute_reply.started":"2022-10-01T13:54:52.699275Z","shell.execute_reply":"2022-10-01T13:54:52.705942Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"torch.Size([1, 7, 1024])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Run a sample input through DEBERTA V3 LARGE model","metadata":{}},{"cell_type":"code","source":"config = AutoConfig.from_pretrained(CFG.DEBERTA_V3_LARGE_PATH, output_hidden_states = True)\nmodel_deberta_v3_large = AutoModel.from_pretrained(CFG.DEBERTA_V3_LARGE_PATH, config = config)\ntokenizer = AutoTokenizer.from_pretrained(CFG.DEBERTA_V3_LARGE_PATH)\n\nsent = \"This is a patent.\"\ntokenized_sent = tokenizer(sent, padding = True, truncation = True, return_tensors = \"pt\")\nprint(tokenized_sent)\nout = model_deberta_v3_large(**tokenized_sent)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-01T13:54:52.708440Z","iopub.execute_input":"2022-10-01T13:54:52.708860Z","iopub.status.idle":"2022-10-01T13:55:07.591870Z","shell.execute_reply.started":"2022-10-01T13:54:52.708811Z","shell.execute_reply":"2022-10-01T13:55:07.590710Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifer.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifer.bias', 'mask_predictions.dense.weight']\n- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"{'input_ids': tensor([[   1,  329,  269,  266, 6581,  260,    2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n","output_type":"stream"}]},{"cell_type":"code","source":"print(out[0])","metadata":{"execution":{"iopub.status.busy":"2022-10-01T13:55:07.593278Z","iopub.execute_input":"2022-10-01T13:55:07.593659Z","iopub.status.idle":"2022-10-01T13:55:07.604746Z","shell.execute_reply.started":"2022-10-01T13:55:07.593622Z","shell.execute_reply":"2022-10-01T13:55:07.603615Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"tensor([[[-0.4624, -0.1580,  0.1821,  ..., -0.0218, -5.3449,  0.0437],\n         [-0.3222,  0.1772, -0.6142,  ...,  0.7221, -0.6977,  0.3440],\n         [ 0.0907,  0.5733, -0.2337,  ...,  0.7742, -0.7838,  0.4246],\n         ...,\n         [ 0.0646, -0.6353,  0.1548,  ...,  1.3551, -0.1917,  1.0668],\n         [-0.1836,  0.2148,  0.4133,  ...,  0.6328,  0.4898,  0.6443],\n         [ 1.5244, -0.5936,  0.5181,  ..., -0.0644,  0.2917,  0.0191]]],\n       grad_fn=<NativeLayerNormBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Build Custom Dataset","metadata":{}},{"cell_type":"code","source":"class PatentClassification_TrainDataset(IterableDataset):\n    def __init__(self, cfg, cpc_code_dict, data_year_ranges, num_batches):\n        '''\n        data_year_ranges can be 1x2 list or 2x2 list\n        every row of the list specifies an year range\n        '''\n        super(PatentClassification_TrainDataset).__init__()\n        self.cfg = cfg\n        self.cpc_code_dict = cpc_code_dict\n        credentials = service_account.Credentials.from_service_account_file(self.cfg.GOOGLE_CLOUD_CRED_JSON)\n        self.bq_client = bigquery.Client(credentials= credentials, project = self.cfg.GOOGLE_CLOUD_PROJ_ID)\n        self.table_row_offset = 0\n        self.text = []\n        self.label_lists = []\n        self.num_batches = num_batches\n        if len(data_year_ranges) == 1:\n            self.data_start_date1 = r'{}-01-01'.format(data_year_ranges[0][0])\n            self.data_end_date1 = r'{}-12-31'.format(data_year_ranges[0][1])\n            self.data_start_date2 = self.data_start_date1\n            self.data_end_date2 = self.data_end_date1\n        else:\n            self.data_start_date1 = r'{}-01-01'.format(data_year_ranges[0][0])\n            self.data_end_date1 = r'{}-12-31'.format(data_year_ranges[0][1])\n            self.data_start_date2 = r'{}-01-01'.format(data_year_ranges[1][0])\n            self.data_end_date2 = r'{}-12-31'.format(data_year_ranges[1][1])\n        \n        if self.data_start_date1 != self.data_start_date2:\n            self.num_table_rows = find_num_dstnct_patent_cpc_in_years(data_year_ranges[0][0], data_year_ranges[0][1]) + \\\n                                  find_num_dstnct_patent_cpc_in_years(data_year_ranges[1][0], data_year_ranges[1][1])\n        else:\n            self.num_table_rows = find_num_dstnct_patent_cpc_in_years(data_year_ranges[0][0], data_year_ranges[0][1])\n            \n        self.f_reach_end = 0\n        \n        \n    def prepare_input(self):\n        inputs = self.cfg.tokenizer(self.text, add_special_tokens=True, max_length=self.cfg.MAX_TOKEN_LEN, padding=\"max_length\",\n                               return_offsets_mapping=False)\n        for k, v in inputs.items():\n            inputs[k] = torch.tensor(v, dtype=torch.long)\n            \n        labels = torch.zeros(len(self.label_lists), self.cfg.NUM_CLASSES, dtype = torch.float)\n        \n        for i, label_list in enumerate(self.label_lists):\n            indices = torch.tensor(list(map(self.cpc_code_dict.__getitem__, label_list)))\n            labels[i].index_fill_(0, indices, 1.0)         \n            \n        return inputs, labels\n            \n                  \n    def __iter__(self):\n        \n        if self.data_start_date1 != self.data_start_date2:\n            query = r\"\"\"\n                    SELECT DISTINCT t1.id, t2.group_id, t1.abstract\n                    FROM `patents-public-data.patentsview.patent` t1, `patents-public-data.patentsview.cpc_current` t2 \n                    WHERE t1.id = t2.patent_id AND t1.type='utility' AND ((timestamp(t1.date) >= timestamp('{}') AND timestamp(t1.date) <= timestamp('{}'))\n                    OR (timestamp(t1.date) >= timestamp('{}') AND timestamp(t1.date) <= timestamp('{}')))\n                    ORDER BY t1.id ASC, t2.group_id\n                    LIMIT {}\n                    OFFSET {}\n                    \"\"\".format(self.data_start_date1, self.data_end_date1, self.data_start_date2, self.data_end_date2, \n                               self.cfg.BATCH_SIZE, self.table_row_offset)\n        else:\n            query = r\"\"\"\n                    SELECT DISTINCT t1.id, t2.group_id, t1.abstract\n                    FROM `patents-public-data.patentsview.patent` t1, `patents-public-data.patentsview.cpc_current` t2 \n                    WHERE t1.id = t2.patent_id AND t1.type='utility' AND timestamp(t1.date) >= timestamp('{}') AND timestamp(t1.date) <= timestamp('{}')\n                    ORDER BY t1.id ASC, t2.group_id\n                    LIMIT {}\n                    OFFSET {}\n                    \"\"\".format(self.data_start_date1, self.data_end_date1, self.cfg.BATCH_SIZE, self.table_row_offset)\n            \n        \n        df = self.bq_client.query(query).to_dataframe()\n        patent_ids = df['id'].unique()\n        for patent_id in patent_ids:\n            df_curr = df.loc[df['id'] == patent_id].reset_index()\n            self.text.append(df_curr['abstract'][0]) #since patent id is same, abstracts are same(basically same patent has multiple cpc codes)\n            self.label_lists.append(list(df_curr['group_id']))\n            \n        inputs, labels = self.prepare_input()\n        \n        self.table_row_offset  = self.table_row_offset + self.cfg.BATCH_SIZE\n        if self.table_row_offset >= self.num_table_rows:\n            self.table_row_offset = 0  #most probably not needed\n            self.f_reach_end = 1\n            \n        self.text = []\n        self.label_lists = []\n        \n        yield inputs, labels, self.f_reach_end\n        \n        \ndef worker_init_fn(_):\n    worker_info = torch.utils.data.get_worker_info()\n    dataset = worker_info.dataset  # the dataset copy in this worker process\n    num_batches_per_worker = dataset.num_batches//worker_info.num_workers\n    worker_id = worker_info.id\n    dataset.table_row_offset = worker_id*num_batches_per_worker*dataset.cfg.BATCH_SIZE\n   ","metadata":{"execution":{"iopub.status.busy":"2022-10-01T13:55:07.606381Z","iopub.execute_input":"2022-10-01T13:55:07.606745Z","iopub.status.idle":"2022-10-01T13:55:07.627627Z","shell.execute_reply.started":"2022-10-01T13:55:07.606708Z","shell.execute_reply":"2022-10-01T13:55:07.626709Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"data_year_ranges = [[2000, 2005]]\nnum_batches = math.ceil(find_num_patents_in_years(data_year_ranges[0][0], data_year_ranges[0][1])/ CFG.BATCH_SIZE)\n\ntrain_dataset = PatentClassification_TrainDataset(CFG, cpc_code_dict, [[2000, 2005]], num_batches)\ntrain_dataloader = DataLoader(train_dataset, batch_size = None, num_workers = 4, worker_init_fn = worker_init_fn)\n\nfor _ in range(1):\n    inputs, labels, f_reached_end = next(iter(train_dataloader))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-10-01T13:55:07.629365Z","iopub.execute_input":"2022-10-01T13:55:07.630013Z","iopub.status.idle":"2022-10-01T13:55:15.105433Z","shell.execute_reply.started":"2022-10-01T13:55:07.629976Z","shell.execute_reply":"2022-10-01T13:55:15.104178Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Build Custom Model","metadata":{}},{"cell_type":"code","source":"class PatentClassificationModel(torch.nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.model_config = AutoConfig.from_pretrained(self.cfg.BERT_FOR_PATENTS_PATH, output_hidden_states=True)\n        self.model = AutoModel.from_pretrained(self.cfg.BERT_FOR_PATENTS_PATH, config = self.model_config) \n        self.attention = torch.nn.Sequential(torch.nn.Linear(self.model_config.hidden_size, self.cfg.ATTENTION_INTERMEDIATE_SIZE),\n                                             torch.nn.Tanh(),\n                                             torch.nn.Linear(self.cfg.ATTENTION_INTERMEDIATE_SIZE, 1),\n                                            torch.nn.Softmax(dim=1))\n        self.fc_dropout = torch.nn.Dropout(self.cfg.DROPOUT_PROB)\n        self.fc = torch.nn.Linear(self.model_config.hidden_size, self.cfg.NUM_CLASSES)\n        \n        \n    def _init_weights(self, module):\n        if isinstance(module, torch.nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, torch.nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, torch.nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, inputs):\n        #one abstract can have multiple cpc codes, hence pass a list of cpc codes for each abstract\n        model_outputs = self.model(**inputs)\n        last_hidden_states = model_outputs.last_hidden_state\n        weights = self.attention(last_hidden_states)\n        weighted_avg_hidden_states = torch.sum(weights * last_hidden_states, dim=1)\n        outputs = self.fc(self.fc_dropout(weighted_avg_hidden_states))\n        return outputs\n        ","metadata":{"execution":{"iopub.status.busy":"2022-10-01T13:55:15.107278Z","iopub.execute_input":"2022-10-01T13:55:15.107694Z","iopub.status.idle":"2022-10-01T13:55:15.122976Z","shell.execute_reply.started":"2022-10-01T13:55:15.107648Z","shell.execute_reply":"2022-10-01T13:55:15.121853Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model = PatentClassificationModel(CFG)","metadata":{"execution":{"iopub.status.busy":"2022-10-01T13:55:15.124504Z","iopub.execute_input":"2022-10-01T13:55:15.125027Z","iopub.status.idle":"2022-10-01T13:55:20.869468Z","shell.execute_reply.started":"2022-10-01T13:55:15.124990Z","shell.execute_reply":"2022-10-01T13:55:20.868522Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at ../input/bert-for-patents/bert-for-patents-pytorch were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Helper functions","metadata":{}},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n        \ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n\ndef get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n         'lr': encoder_lr, 'weight_decay': weight_decay},\n        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n         'lr': encoder_lr, 'weight_decay': 0.0},\n        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n         'lr': decoder_lr, 'weight_decay': 0.0}\n    ]\n    return optimizer_parameters\n\n\n    \n# ====================================================\n# scheduler\n# ====================================================\ndef get_scheduler(cfg, optimizer, num_train_steps):\n    if cfg.SCHEDULER == 'linear':\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, num_warmup_steps=cfg.NUM_WARMUP_STEPS, num_training_steps=num_train_steps\n        )\n    elif cfg.SCHEDULER == 'cosine':\n        scheduler = get_cosine_schedule_with_warmup(\n            optimizer, num_warmup_steps=cfg.NUM_WARMUP_STEPS, num_training_steps=num_train_steps, num_cycles=cfg.NUM_CYCLES\n        )\n    return scheduler\n\n\n# ====================================================\n# Function to calculate loss\n# ====================================================\ndef get_loss(preds, labels, criterion):\n    with torch.autocast(device_type = device.type):\n        loss = criterion(preds.view(-1, 1), labels.view(-1, 1))  \n        loss = loss.item()\n    return loss\n    \n\ndef train_fn(train_dataloader, num_batches, model, criterion, optimizer, epoch, scheduler, device):\n    \n    model.train()\n    scaler = torch.cuda.amp.GradScaler()\n    loss_info = AverageMeter()\n    f_end_reached = 0\n    step_num = 0\n    start = end = time.time()\n    \n    while not f_end_reached:\n        \n        step_num = step_num + 1\n        inputs, labels, f_end_reached = next(iter(train_dataloader))\n        \n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        labels = labels.to(device)\n        \n        batch_size = labels.size(0)\n        \n        with torch.autocast(device_type = device.type):\n            outputs = model(inputs)\n            loss = criterion(outputs.view(-1, 1), labels.view(-1, 1))\n            \n        loss_info.update(loss.item(), batch_size)\n                \n        # Set all gradients to 0\n        optimizer.zero_grad()\n        \n        # Calculate and scale gradients\n        scaler.scale(loss).backward()\n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.MAX_GRAD_NORM)\n        \n        # Update Weights and Biases, Scaler\n        scaler.step(optimizer)\n        scaler.update()\n        \n        #Update Scheduler\n        scheduler.step()\n        \n        if step_num % CFG.PRINT_FREQ == 0 or step_num == num_batches - 1:\n            print('Epoch: [{0}][{1}/{2}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  'Grad: {grad_norm:.4f}  '\n                  'LR: {lr:.8f}  '\n                  .format(epoch+1, step_num, num_batches, \n                          remain=timeSince(start, float(step_num)/num_batches),\n                          loss=loss_info,\n                          grad_norm=grad_norm,\n                          lr=scheduler.get_lr()[0]))\n    \n    return loss_info.avg\n\ndef valid_fn(valid_dataloader, num_batches, model, criterion, device):\n    \n    model.eval()\n    loss_info = AverageMeter()\n    f_end_reached = 0\n    step_num = 0\n    start = end = time.time()\n    \n    while not f_end_reached:\n        \n        step_num = step_num + 1\n        inputs, labels, f_end_reached = next(iter(valid_dataloader))\n        \n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        labels = labels.to(device)\n        \n        batch_size = labels.size(0)\n        \n        with torch.autocast(device_type = device.type):\n            with torch.no_grad():\n                outputs = model(inputs)\n            loss = criterion(outputs.view(-1, 1), labels.view(-1, 1))      \n                    \n        loss_info.update(loss.item(), batch_size)\n        try:\n            preds_concat = torch.cat((preds_concat, outputs), dim = 0)\n            labels_concat = torch.cat((labels_concat, labels), dim = 0)\n        except:    \n            preds_concat = outputs\n            labels_concat = labels\n        \n        if step_num % CFG.PRINT_FREQ == 0 or step_num == num_batches - 1:\n            print('Epoch: [{0}][{1}/{2}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  .format(epoch+1, step_num, num_batches, \n                          remain=timeSince(start, float(step_num)/num_batches),\n                          loss=loss_info))\n        \n        \n        return loss_info.avg, preds_concat, labels_concat\n    \n    \n\n                    \n ","metadata":{"execution":{"iopub.status.busy":"2022-10-01T13:55:20.871169Z","iopub.execute_input":"2022-10-01T13:55:20.871582Z","iopub.status.idle":"2022-10-01T13:55:20.898049Z","shell.execute_reply.started":"2022-10-01T13:55:20.871523Z","shell.execute_reply":"2022-10-01T13:55:20.896922Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"### Training loop","metadata":{}},{"cell_type":"code","source":"def train_loop(fold):\n    \n    train_year_ranges = [[0, 0], [0, 0]]\n    valid_year_range = [[0, 0]]\n    fold_year_range_len = (CFG.TRAIN_PATENT_END_YEAR - (CFG.TRAIN_PATENT_START_YEAR - 1))/CFG.NUM_FOLDS\n    valid_year_range[0][0] = int(CFG.TRAIN_PATENT_START_YEAR + (fold - 1)*fold_year_range_len)\n    valid_year_range[0][1] = int(valid_year_range[0][0] + (fold_year_range_len - 1))\n    if fold == 1:\n        train_year_ranges[0][0] = int(valid_year_range[0][1] + 1)\n        train_year_ranges[0][1] = int(CFG.TRAIN_PATENT_END_YEAR)\n        train_year_ranges[1][0] = train_year_ranges[0][0] \n        train_year_ranges[1][1] = train_year_ranges[0][1]    \n    else:\n        train_year_ranges[0][0] = int(CFG.TRAIN_PATENT_START_YEAR)\n        train_year_ranges[0][1] = int(valid_year_range[0][0] - 1)\n        train_year_ranges[1][0] = int(valid_year_range[0][1] + 1)\n        train_year_ranges[1][1] = int(CFG.TRAIN_PATENT_END_YEAR)\n\n    \n    model = PatentClassificationModel(CFG)\n    model.to(device)\n    \n    optimizer_parameters = get_optimizer_params(model,\n                                                encoder_lr=CFG.ENCODER_LR, \n                                                decoder_lr=CFG.DECODER_LR,\n                                                weight_decay=CFG.WEIGHT_DECAY)\n    \n    optimizer = AdamW(optimizer_parameters, lr=CFG.ENCODER_LR, eps=CFG.EPS, betas=CFG.BETAS)\n    if fold == 1:\n        num_train_batches = math.ceil(find_num_patents_in_years(train_year_ranges[0][0], train_year_ranges[0][1])/ CFG.BATCH_SIZE)\n    else:\n        num_train_batches = math.ceil((find_num_patents_in_years(train_year_ranges[0][0], train_year_ranges[0][1]) + \\\n                                 find_num_patents_in_years(train_year_ranges[1][0], train_year_ranges[1][1]))/ CFG.BATCH_SIZE)\n    num_val_batches = math.ceil(find_num_patents_in_years(valid_year_range[0][0], valid_year_range[0][1])/ CFG.BATCH_SIZE)\n        \n    \n    num_train_steps = num_batches * CFG.NUM_EPOCHS\n    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n    \n    # ====================================================\n    # loop\n    # ====================================================\n    criterion = torch.nn.BCEWithLogitsLoss(reduction=\"mean\")\n    \n    best_loss = CFG.INFINITY\n    best_epoch = 0\n    \n    train_dataset = PatentClassification_TrainDataset(CFG, cpc_code_dict, train_year_ranges, num_train_batches)\n    valid_dataset = PatentClassification_TrainDataset(CFG, cpc_code_dict, valid_year_range, num_val_batches)\n    \n    #num_workers = 2 gives best speed (for GPU)\n    train_dataloader = DataLoader(train_dataset, batch_size = None, num_workers = 2, worker_init_fn = worker_init_fn) #batches are created in dataset itself\n    valid_dataloader = DataLoader(valid_dataset, batch_size = None, num_workers = 2, worker_init_fn = worker_init_fn) #batches are created in dataset itself\n    \n    \n    for epoch in range(CFG.NUM_EPOCHS):\n        start_time = time.time()\n        \n        #train\n        avg_train_loss = train_fn(train_dataloader, num_train_batches, model, criterion, optimizer, epoch, scheduler, device)\n        \n        #eval\n        avg_val_loss, preds_val, labels_val = valid_fn(valid_dataloader, num_val_batches, model, criterion, device)\n        \n        #overall validation loss\n        overall_val_loss = get_loss(preds_concat, labels_concat, criterion)\n        \n        elapsed = time.time() - start_time\n        \n        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_train_loss:.4f},  avg_val_loss: {avg_val_loss:.4f},  overall_val_loss: {overall_val_loss: 4f} time: {elapsed:.0f}s\")\n            \n        if overall_val_loss < best_loss:\n            best_loss = overall_val_loss\n            best_epoch = epoch + 1\n            best_epoch_preds_val = preds_val\n            best_epoch_labels_val = labels_val\n            print(f'Epoch {best_epoch} - Save Best Score: {best_score:.4f} Model')\n            torch.save({'model': model.state_dict()},\n                       OUTPUT_DIR+f\"model_fold{fold}_best.pth\")\n    \n          \n            \n    torch.cuda.empty_cache()\n    gc.collect()\n              \n    return best_epoch_preds_val, best_epoch_labels_val ","metadata":{"execution":{"iopub.status.busy":"2022-10-01T13:55:20.899779Z","iopub.execute_input":"2022-10-01T13:55:20.900526Z","iopub.status.idle":"2022-10-01T13:55:20.920310Z","shell.execute_reply.started":"2022-10-01T13:55:20.900486Z","shell.execute_reply":"2022-10-01T13:55:20.919293Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    \n    if CFG.F_TRAIN:\n        \n        for fold in range(1, CFG.NUM_FOLDS, 1):\n            best_epoch_preds_val, best_epoch_labels_val = train_loop(fold)\n            \n            if fold != 1:\n                concat_preds_val = torch.cat((concat_preds_val, best_epoch_preds_val), dim = 0)\n                concat_labels_val = torch.cat((concat_labels_val, best_epoch_labels_val), dim = 0)\n            else:\n                concat_preds_val = best_epoch_preds_val\n                concat_labels_val = best_epoch_labels_val\n                \n            fold_val_loss = get_loss(best_epoch_preds_val, best_epoch_labels_val, criterion)\n            print(f\"Fold {fold} validation loss: {fold_val_loss:.4f}\")\n            \n        overall_val_loss = get_loss(concat_preds_val, concat_labels_val, criterion)\n        print(f\"Overall Cross Validation loss: {overall_val_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2022-10-01T13:55:20.923042Z","iopub.execute_input":"2022-10-01T13:55:20.923899Z","iopub.status.idle":"2022-10-01T13:57:22.755382Z","shell.execute_reply.started":"2022-10-01T13:55:20.923859Z","shell.execute_reply":"2022-10-01T13:57:22.752361Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at ../input/bert-for-patents/bert-for-patents-pytorch were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Epoch: [1][5/20925] Elapsed 0m 20s (remain 1442m 23s) Loss: 0.6530(0.6781) Grad: 25437.0781  LR: 0.00002000  \nEpoch: [1][10/20925] Elapsed 0m 34s (remain 1190m 23s) Loss: 0.5858(0.6459) Grad: 42732.9102  LR: 0.00002000  \nEpoch: [1][15/20925] Elapsed 0m 46s (remain 1088m 51s) Loss: 0.5168(0.6120) Grad: 32012.1699  LR: 0.00002000  \nEpoch: [1][20/20925] Elapsed 0m 59s (remain 1032m 25s) Loss: 0.4575(0.5798) Grad: 31056.2676  LR: 0.00002000  \nEpoch: [1][25/20925] Elapsed 1m 11s (remain 996m 46s) Loss: 0.4162(0.5502) Grad: 27193.9805  LR: 0.00002000  \nEpoch: [1][30/20925] Elapsed 1m 23s (remain 973m 33s) Loss: 0.3811(0.5239) Grad: 25830.4980  LR: 0.00002000  \nEpoch: [1][35/20925] Elapsed 1m 35s (remain 953m 55s) Loss: 0.3540(0.5011) Grad: 23612.6582  LR: 0.00002000  \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/406752723.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNUM_FOLDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mbest_epoch_preds_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_epoch_labels_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_17/149133362.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(fold)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m#train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_train_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m#eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_17/102288952.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(train_dataloader, num_batches, model, criterion, optimizer, epoch, scheduler, device)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mloss_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# Set all gradients to 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}