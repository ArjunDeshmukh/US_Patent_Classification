{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Importing Libraries\n","metadata":{"id":"83YXgK8eC0_B"}},{"cell_type":"code","source":"try:\n    import google.colab\n    IN_COLAB = True\nexcept:\n    IN_COLAB = False","metadata":{"id":"6x774kWbGeJI","execution":{"iopub.status.busy":"2022-10-16T16:43:11.078339Z","iopub.execute_input":"2022-10-16T16:43:11.079466Z","iopub.status.idle":"2022-10-16T16:43:11.104544Z","shell.execute_reply.started":"2022-10-16T16:43:11.079349Z","shell.execute_reply":"2022-10-16T16:43:11.103463Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"if IN_COLAB:\n    from google.colab import drive\n    drive.mount('/content/drive')","metadata":{"id":"69ZldWGCDQ7n","outputId":"79b2851e-64f9-4cac-a904-598c5e4cba68","execution":{"iopub.status.busy":"2022-10-16T16:43:11.109555Z","iopub.execute_input":"2022-10-16T16:43:11.110209Z","iopub.status.idle":"2022-10-16T16:43:11.115693Z","shell.execute_reply.started":"2022-10-16T16:43:11.110181Z","shell.execute_reply":"2022-10-16T16:43:11.114707Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"if IN_COLAB:\n    !pip install transformers","metadata":{"id":"PjYyyi9tGGEL","outputId":"0b1c893c-509b-4d89-87a9-c2b17d85c077","execution":{"iopub.status.busy":"2022-10-16T16:43:11.173381Z","iopub.execute_input":"2022-10-16T16:43:11.173680Z","iopub.status.idle":"2022-10-16T16:43:11.177899Z","shell.execute_reply.started":"2022-10-16T16:43:11.173654Z","shell.execute_reply":"2022-10-16T16:43:11.176900Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import requests\nimport json\nimport sys\nimport pandas as pd\nimport os.path\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\n\nimport time\nfrom tqdm import tqdm\n\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\n\nfrom transformers import AutoModelForMaskedLM\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, DataCollatorWithPadding\n\nimport torch\nfrom torch.profiler import profile, record_function, ProfilerActivity\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import Dataset, DataLoader, IterableDataset\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")   \n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Device: \", device)\n\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nfrom sys import getsizeof\nimport gc","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"e7bR6yPtC0_E","execution":{"iopub.status.busy":"2022-10-16T16:43:11.201385Z","iopub.execute_input":"2022-10-16T16:43:11.201692Z","iopub.status.idle":"2022-10-16T16:43:17.895987Z","shell.execute_reply.started":"2022-10-16T16:43:11.201666Z","shell.execute_reply":"2022-10-16T16:43:17.894938Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Device:  cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Define configuration constants","metadata":{"id":"FZvI20qjC0_G"}},{"cell_type":"code","source":"class CFG:\n    CPC_CODES_PATH = \"../input/cpc-code-upto-subclass/CPC_codes_upto_subclass.csv\"\n    PATENTS_DATA_PATH = \"../input/us-patents-abstracts-cpc/US_Patents_Abstracts_CPC.csv\"\n    BERT_FOR_PATENTS_PATH = \"../input/bert-for-patents/bert-for-patents-pytorch\"\n    DEBERTA_V3_LARGE_PATH = \"../input/deberta-v3-large/deberta-v3-large\"\n    CPC_CODES_NUM_PATENTS_PATH = \"../input/cpc-code-num-patents/cpc_code_num_patents.csv\"\n    MAX_TOKEN_LEN = 128\n    DROPOUT_PROB = 0.2\n    ATTENTION_INTERMEDIATE_SIZE = 512\n    GOOGLE_CLOUD_CRED_JSON = '../input/googlecloudcred/us-patent-classification-d344a6ddc702.json'\n    GOOGLE_CLOUD_PROJ_ID = 'us-patent-classification'\n    BATCH_SIZE = 16\n    NUM_EPOCHS = 5\n    TRAIN_PATENT_START_YEAR = 2001\n    TRAIN_PATENT_END_YEAR = 2018\n    TEST_PATENT_START_YEAR = 2019\n    TEST_PATENT_END_YEAR = 2022\n    NUM_FOLDS = 6 #Total number of years used for training should be divisible by this number\n    ENCODER_LR = 2e-5\n    DECODER_LR = 2e-5\n    MIN_LR = 1e-6\n    EPS = 1e-6\n    WEIGHT_DECAY = 0.01\n    SCHEDULER = 'linear'\n    NUM_WARMUP_STEPS = 0\n    NUM_CYCLES = 0.5\n    BETAS=(0.9, 0.999)\n    MAX_GRAD_NORM = 1000\n    PRINT_FREQ = 5\n    INFINITY = 1e6\n    F_TRAIN = 1\n  \n  # Parameters which will be added in subsequent code:\n  # NUM_CLASSES, TRAIN_NUM_PATENTS\n\nif IN_COLAB:\n    CFG.CPC_CODES_PATH = \"/content/drive/MyDrive/MachineLearning/ML_Deployment/Patent_Class_Inputs/cpc-code-upto-subclass/CPC_codes_upto_subclass.csv\"\n    CFG.BERT_FOR_PATENTS_PATH = \"/content/drive/MyDrive/MachineLearning/ML_Deployment/Patent_Class_Inputs/bert-for-patents-pytorch\"\n    CFG.GOOGLE_CLOUD_CRED_JSON = \"/content/drive/MyDrive/MachineLearning/ML_Deployment/Patent_Class_Inputs/googlecloudcred/us-patent-classification-d344a6ddc702.json\"","metadata":{"id":"TnAfPfmSC0_G","execution":{"iopub.status.busy":"2022-10-16T16:43:17.899416Z","iopub.execute_input":"2022-10-16T16:43:17.899906Z","iopub.status.idle":"2022-10-16T16:43:17.908575Z","shell.execute_reply.started":"2022-10-16T16:43:17.899879Z","shell.execute_reply":"2022-10-16T16:43:17.907571Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### List of CPC codes","metadata":{"id":"07hZ1QpUC0_H"}},{"cell_type":"code","source":"cpc_codes_upto_subclass_df = pd.read_csv(CFG.CPC_CODES_PATH)\ncpc_codes_upto_subclass_df.head()\nnum_codes = len(pd.unique(cpc_codes_upto_subclass_df['code']))\ndf_len = len(cpc_codes_upto_subclass_df)\nCFG.NUM_CLASSES = df_len\nprint(\"Number of rows in dataframe: \", df_len, \"\\n\")\nprint(\"Number of CPC codes upto subclass: \", num_codes)\ncpc_code_dict = dict(zip(cpc_codes_upto_subclass_df['code'].values, cpc_codes_upto_subclass_df.index))","metadata":{"id":"v_Bg2DLLC0_H","outputId":"fc16f659-9965-44cb-cf2b-0b4c965ef705","execution":{"iopub.status.busy":"2022-10-16T16:43:17.910404Z","iopub.execute_input":"2022-10-16T16:43:17.911181Z","iopub.status.idle":"2022-10-16T16:43:17.941915Z","shell.execute_reply.started":"2022-10-16T16:43:17.911145Z","shell.execute_reply":"2022-10-16T16:43:17.940986Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Number of rows in dataframe:  674 \n\nNumber of CPC codes upto subclass:  674\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Dowloaded Patent Data","metadata":{}},{"cell_type":"code","source":"patent_data_df = pd.read_csv(CFG.PATENTS_DATA_PATH)\npatent_data_df.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis=1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-10-16T16:43:17.944606Z","iopub.execute_input":"2022-10-16T16:43:17.945214Z","iopub.status.idle":"2022-10-16T16:43:43.107395Z","shell.execute_reply.started":"2022-10-16T16:43:17.945178Z","shell.execute_reply":"2022-10-16T16:43:43.106390Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#patent_data_agg_df = patent_data_df.groupby(['id', 'abstract'], sort = False, as_index = False).agg({'group_id':list})","metadata":{"execution":{"iopub.status.busy":"2022-10-16T16:43:43.108826Z","iopub.execute_input":"2022-10-16T16:43:43.109189Z","iopub.status.idle":"2022-10-16T16:43:43.115009Z","shell.execute_reply.started":"2022-10-16T16:43:43.109149Z","shell.execute_reply":"2022-10-16T16:43:43.113892Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Count the number of patents for each cpc_code","metadata":{"id":"2EZi48ItC0_L"}},{"cell_type":"code","source":"'''\ncpc_code_count = dict(zip(cpc_codes_upto_subclass_df.index, [0 for x in range(0,len(cpc_codes_upto_subclass_df.index))]))\n\nfor cpc_code in cpc_code_dict.keys():\n    \n    cpc_code_count[cpc_code_dict[cpc_code]] = len(patent_data_df[patent_data_df['group_id'] == cpc_code].index)\n    \nplt.plot(cpc_code_count.keys(), cpc_code_count.values())\n\n'''\n","metadata":{"execution":{"iopub.status.busy":"2022-10-16T16:43:43.116894Z","iopub.execute_input":"2022-10-16T16:43:43.117183Z","iopub.status.idle":"2022-10-16T16:43:43.129453Z","shell.execute_reply.started":"2022-10-16T16:43:43.117158Z","shell.execute_reply":"2022-10-16T16:43:43.128255Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"\"\\ncpc_code_count = dict(zip(cpc_codes_upto_subclass_df.index, [0 for x in range(0,len(cpc_codes_upto_subclass_df.index))]))\\n\\nfor cpc_code in cpc_code_dict.keys():\\n    \\n    cpc_code_count[cpc_code_dict[cpc_code]] = len(patent_data_df[patent_data_df['group_id'] == cpc_code].index)\\n    \\nplt.plot(cpc_code_count.keys(), cpc_code_count.values())\\n\\n\""},"metadata":{}}]},{"cell_type":"markdown","source":"### Train Validation Split","metadata":{}},{"cell_type":"code","source":"# ================================================================================================\n# train_test_split does not work if the \"stratify\" input has any label with only a single occurence\n# So, we need to repeat those rows which have CPC codes with just a single occurence\n# =================================================================================================\ns = patent_data_df['group_id'].value_counts()\nfreq_1_s = s[s == 1]\nfor group_id in freq_1_s.index:\n    s2 = patent_data_df[patent_data_df['group_id'] == group_id]\n    patent_data_df = pd.concat([patent_data_df, s2], ignore_index = True)","metadata":{"execution":{"iopub.status.busy":"2022-10-16T16:43:43.131054Z","iopub.execute_input":"2022-10-16T16:43:43.132302Z","iopub.status.idle":"2022-10-16T16:43:43.709085Z","shell.execute_reply.started":"2022-10-16T16:43:43.132266Z","shell.execute_reply":"2022-10-16T16:43:43.708115Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_df, valid_df = train_test_split(patent_data_df, test_size=0.2, stratify = patent_data_df['group_id'])\n\ntrain_df.reset_index(inplace = True)\nvalid_df.reset_index(inplace = True)\n\ntrain_df = train_df.groupby(['id', 'abstract'], sort = False, as_index = False).agg({'group_id':list})\nvalid_df = valid_df.groupby(['id', 'abstract'], sort = False, as_index = False).agg({'group_id':list})","metadata":{"execution":{"iopub.status.busy":"2022-10-16T16:43:43.710642Z","iopub.execute_input":"2022-10-16T16:43:43.711285Z","iopub.status.idle":"2022-10-16T16:43:57.806269Z","shell.execute_reply.started":"2022-10-16T16:43:43.711245Z","shell.execute_reply":"2022-10-16T16:43:57.805266Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Run a sample input through the BERT for Patents model","metadata":{"id":"YdK5TxBMC0_P"}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(CFG.BERT_FOR_PATENTS_PATH)\nCFG.tokenizer = tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-10-16T16:43:57.807871Z","iopub.execute_input":"2022-10-16T16:43:57.808242Z","iopub.status.idle":"2022-10-16T16:43:57.908203Z","shell.execute_reply.started":"2022-10-16T16:43:57.808203Z","shell.execute_reply":"2022-10-16T16:43:57.907287Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"config = AutoConfig.from_pretrained(CFG.BERT_FOR_PATENTS_PATH)\nconfig.max_position_embeddings = CFG.MAX_TOKEN_LEN\nmodel_bert_for_patents = AutoModel.from_pretrained(CFG.BERT_FOR_PATENTS_PATH, config = config, ignore_mismatched_sizes=True)\n\nsent = \"This is a patent\"\ntokenized_sent = tokenizer(sent, truncation = True, add_special_tokens=True, max_length=CFG.MAX_TOKEN_LEN, padding=\"max_length\",\n                           return_offsets_mapping=False, return_tensors = \"pt\")\nout = model_bert_for_patents(**tokenized_sent)","metadata":{"id":"WqloBp8DC0_P","outputId":"5f695dd0-86a2-417d-87af-aeba6c586f2e","execution":{"iopub.status.busy":"2022-10-16T16:43:57.911797Z","iopub.execute_input":"2022-10-16T16:43:57.912088Z","iopub.status.idle":"2022-10-16T16:44:15.492178Z","shell.execute_reply.started":"2022-10-16T16:43:57.912061Z","shell.execute_reply":"2022-10-16T16:44:15.491130Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at ../input/bert-for-patents/bert-for-patents-pytorch were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertModel were not initialized from the model checkpoint at ../input/bert-for-patents/bert-for-patents-pytorch and are newly initialized because the shapes did not match:\n- bert.embeddings.position_ids: found shape torch.Size([1, 512]) in the checkpoint and torch.Size([1, 128]) in the model instantiated\n- bert.embeddings.position_embeddings.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([128, 1024]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"config","metadata":{"execution":{"iopub.status.busy":"2022-10-16T16:44:15.493987Z","iopub.execute_input":"2022-10-16T16:44:15.494992Z","iopub.status.idle":"2022-10-16T16:44:15.503614Z","shell.execute_reply.started":"2022-10-16T16:44:15.494947Z","shell.execute_reply":"2022-10-16T16:44:15.502449Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"BertConfig {\n  \"_name_or_path\": \"../input/bert-for-patents/bert-for-patents-pytorch\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 128,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 39859\n}"},"metadata":{}}]},{"cell_type":"markdown","source":"### Build Custom Dataset","metadata":{"id":"R0mEO5oIC0_Q"}},{"cell_type":"code","source":"class PatentClassification_TrainDataset(Dataset):\n    \n    def __init__(self, cfg, cpc_code_dict, df):\n        super(PatentClassification_TrainDataset).__init__()\n        self.cfg = cfg\n        self.cpc_code_dict = cpc_code_dict\n        self.df = df\n        \n    def prepare_input(self, idx):\n        inputs = self.cfg.tokenizer(self.df.loc[idx, 'abstract'], truncation = True, add_special_tokens=True, \n                                    max_length=self.cfg.MAX_TOKEN_LEN) #Padding will be done in collate_fn\n        \n        for k, v in inputs.items():\n            inputs[k] = torch.tensor(v, dtype=torch.long)\n            \n        labels = torch.zeros(1, self.cfg.NUM_CLASSES, dtype = torch.float)\n        indices = torch.tensor(list(map(self.cpc_code_dict.__getitem__, self.df.loc[idx, 'group_id'])))\n        labels.index_fill_(dim = -1, index = indices, value = 1.0)         \n            \n        return inputs, labels\n    \n    def __len__(self):\n        return len(self.df.index)\n    \n    def __getitem__(self, idx):\n        \n        inputs, labels = self.prepare_input(idx)\n        \n        return inputs, labels\n    \n\ndata_collator = DataCollatorWithPadding(tokenizer=CFG.tokenizer)    \ndef custom_collate_fn(batch):\n    \n    for idx, this_batch in enumerate(batch):\n        inputs, labels = this_batch\n        if idx != 0:\n            inputs_dict_list.append(inputs)\n            labels_list = torch.cat((labels_list, labels), dim=0)\n        else:\n            inputs_dict_list = [inputs]\n            labels_list = labels\n           \n    padded_input_dict_list = data_collator(inputs_dict_list)\n    #print(labels_list.size())\n    return padded_input_dict_list, labels_list\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2022-10-16T16:44:15.505232Z","iopub.execute_input":"2022-10-16T16:44:15.505602Z","iopub.status.idle":"2022-10-16T16:44:15.519982Z","shell.execute_reply.started":"2022-10-16T16:44:15.505564Z","shell.execute_reply":"2022-10-16T16:44:15.518607Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Build Custom Model","metadata":{"id":"TNRQy4gPC0_R"}},{"cell_type":"code","source":"class PatentClassificationModel(torch.nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.model_config = AutoConfig.from_pretrained(self.cfg.BERT_FOR_PATENTS_PATH, output_hidden_states=False)\n        self.model = AutoModel.from_pretrained(self.cfg.BERT_FOR_PATENTS_PATH, config = self.model_config) \n        '''\n        self.attention = torch.nn.Sequential(torch.nn.Linear(self.model_config.hidden_size, self.cfg.ATTENTION_INTERMEDIATE_SIZE),\n                                             torch.nn.Tanh(),\n                                             torch.nn.Linear(self.cfg.ATTENTION_INTERMEDIATE_SIZE, 1),\n                                            torch.nn.Softmax(dim=1))\n        self.fc_dropout = torch.nn.Dropout(self.cfg.DROPOUT_PROB)\n        '''\n        self.fc = torch.nn.Linear(self.model_config.hidden_size, self.cfg.NUM_CLASSES)\n        self._init_weights(self.fc)\n        \n        \n    def _init_weights(self, module):\n        if isinstance(module, torch.nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, torch.nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, torch.nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, inputs):\n        #one abstract can have multiple cpc codes, hence pass a list of cpc codes for each abstract\n        model_outputs = self.model(**inputs)\n        outputs = model_outputs.pooler_output\n        outputs = self.fc(outputs)\n        return outputs\n        ","metadata":{"execution":{"iopub.status.busy":"2022-10-16T16:44:15.521617Z","iopub.execute_input":"2022-10-16T16:44:15.522110Z","iopub.status.idle":"2022-10-16T16:44:15.534801Z","shell.execute_reply.started":"2022-10-16T16:44:15.522076Z","shell.execute_reply":"2022-10-16T16:44:15.533516Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class PatentClassificationModel_old(torch.nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.model_config = AutoConfig.from_pretrained(self.cfg.BERT_FOR_PATENTS_PATH, output_hidden_states=True)\n        self.model = AutoModel.from_pretrained(self.cfg.BERT_FOR_PATENTS_PATH, config = self.model_config, ignore_mismatched_sizes=True) \n        self.attention = torch.nn.Sequential(torch.nn.Linear(self.model_config.hidden_size, self.cfg.ATTENTION_INTERMEDIATE_SIZE),\n                                             torch.nn.Tanh(),\n                                             torch.nn.Linear(self.cfg.ATTENTION_INTERMEDIATE_SIZE, 1),\n                                            torch.nn.Softmax(dim=1))\n        self.fc_dropout = torch.nn.Dropout(self.cfg.DROPOUT_PROB)\n        self.fc = torch.nn.Linear(self.model_config.hidden_size, self.cfg.NUM_CLASSES)\n        \n        \n    def _init_weights(self, module):\n        if isinstance(module, torch.nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, torch.nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, torch.nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, inputs):\n        #one abstract can have multiple cpc codes, hence pass a list of cpc codes for each abstract\n        model_outputs = self.model(**inputs)\n        last_hidden_states = model_outputs.last_hidden_state\n        weights = self.attention(last_hidden_states)\n        weighted_avg_hidden_states = torch.sum(weights * last_hidden_states, dim=1)\n        outputs = self.fc(self.fc_dropout(weighted_avg_hidden_states))\n        return outputs\n        ","metadata":{"id":"RXgveiDPC0_R","execution":{"iopub.status.busy":"2022-10-16T16:44:15.536154Z","iopub.execute_input":"2022-10-16T16:44:15.536734Z","iopub.status.idle":"2022-10-16T16:44:15.550409Z","shell.execute_reply.started":"2022-10-16T16:44:15.536699Z","shell.execute_reply":"2022-10-16T16:44:15.549350Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_dataset = PatentClassification_TrainDataset(CFG, cpc_code_dict, train_df)\ntrain_dataloader = DataLoader(train_dataset, batch_size = CFG.BATCH_SIZE, collate_fn = custom_collate_fn)\n\ninputs, labels = train_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2022-10-16T16:44:15.551802Z","iopub.execute_input":"2022-10-16T16:44:15.552167Z","iopub.status.idle":"2022-10-16T16:44:15.582327Z","shell.execute_reply.started":"2022-10-16T16:44:15.552115Z","shell.execute_reply":"2022-10-16T16:44:15.581474Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### Helper functions","metadata":{"id":"oO3ENUylC0_R"}},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n        \ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n\ndef get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n         'lr': encoder_lr, 'weight_decay': weight_decay},\n        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n         'lr': encoder_lr, 'weight_decay': 0.0},\n        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n         'lr': decoder_lr, 'weight_decay': 0.0}\n    ]\n    return optimizer_parameters\n\n\n    \n# ====================================================\n# scheduler\n# ====================================================\ndef get_scheduler(cfg, optimizer, num_train_steps):\n    if cfg.SCHEDULER == 'linear':\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, num_warmup_steps=cfg.NUM_WARMUP_STEPS, num_training_steps=num_train_steps\n        )\n    elif cfg.SCHEDULER == 'cosine':\n        scheduler = get_cosine_schedule_with_warmup(\n            optimizer, num_warmup_steps=cfg.NUM_WARMUP_STEPS, num_training_steps=num_train_steps, num_cycles=cfg.NUM_CYCLES\n        )\n    return scheduler\n\n\n# ====================================================\n# Function to calculate loss\n# ====================================================\ndef get_loss(preds, labels, criterion):\n    with torch.autocast(device_type = device.type):\n        loss = criterion(preds.view(-1, 1), labels.view(-1, 1))  \n        loss = loss.item()\n    return loss\n    \n\ndef train_fn(train_dataloader, num_batches, model, criterion, optimizer, epoch, scheduler, device):\n    \n    model.train()\n    scaler = torch.cuda.amp.GradScaler()\n    loss_info = AverageMeter()\n    start = end = time.time()\n    \n    for idx, batch in enumerate(train_dataloader):\n                \n        inputs, labels = batch\n        \n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        labels = labels.to(device)\n        \n        batch_size = labels.size(0)\n        \n        model_run_start = time.time()\n        with torch.autocast(device_type = device.type):\n            outputs = model(inputs)\n            loss = criterion(outputs.view(-1, 1), labels.view(-1, 1))\n            \n        loss_info.update(loss.item(), batch_size)\n        \n        # Calculate and scale gradients\n        scaler.scale(loss).backward()\n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.MAX_GRAD_NORM)\n        \n        # Update Weights and Biases, Scaler\n        scaler.step(optimizer)\n        scaler.update()\n        \n        #Update Scheduler\n        scheduler.step()\n        \n        # Set all gradients to 0\n        optimizer.zero_grad(set_to_none=True)\n        model_run_end = time.time()\n        print(\"Model run time: \", model_run_end - model_run_start, \" s\")\n        \n        if idx % CFG.PRINT_FREQ == 0 or idx == num_batches - 1:\n            print('Epoch: [{0}][{1}/{2}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  'Grad: {grad_norm:.4f}  '\n                  'LR: {lr:.8f}  '\n                  .format(epoch+1, idx, num_batches, \n                          remain=timeSince(start, float(idx+1)/num_batches),\n                          loss=loss_info,\n                          grad_norm=grad_norm,\n                          lr=scheduler.get_lr()[0]))\n        torch.cuda.empty_cache()\n    \n    return loss_info.avg\n\ndef valid_fn(valid_dataloader, num_batches, model, criterion, device):\n    \n    model.eval()\n    loss_info = AverageMeter()\n    start = end = time.time()\n    \n    for idx, batch in enumerate(valid_dataloader):\n                \n        inputs, labels = batch\n        \n        batch_size = labels.size(0)\n        \n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        labels = labels.to(device)\n        \n        \n        with torch.autocast(device_type = device.type):\n            with torch.no_grad():\n                outputs = model(inputs)\n            loss = criterion(outputs.view(-1, 1), labels.view(-1, 1))      \n                    \n        loss_info.update(loss.item(), batch_size)\n        try:\n            preds_concat = torch.cat((preds_concat, outputs), dim = 0)\n            labels_concat = torch.cat((labels_concat, labels), dim = 0)\n        except:    \n            preds_concat = outputs\n            labels_concat = labels\n        \n        if idx % CFG.PRINT_FREQ == 0 or idx == num_batches - 1:\n            print('Epoch: [{0}][{1}/{2}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  .format(epoch+1, idx, num_batches, \n                          remain=timeSince(start, float(idx+1)/num_batches),\n                          loss=loss_info))\n        \n        \n        overall_val_loss = get_loss(preds_concat, labels_concat, criterion)\n        \n        torch.cuda.empty_cache()\n        \n        return overall_val_loss","metadata":{"id":"R3vv8eHfC0_S","execution":{"iopub.status.busy":"2022-10-16T16:44:15.583835Z","iopub.execute_input":"2022-10-16T16:44:15.584177Z","iopub.status.idle":"2022-10-16T16:44:15.611328Z","shell.execute_reply.started":"2022-10-16T16:44:15.584144Z","shell.execute_reply":"2022-10-16T16:44:15.610488Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### Training loop","metadata":{"id":"tg78r61EC0_S"}},{"cell_type":"code","source":"def train_loop():\n        \n    model = PatentClassificationModel(CFG)\n    model.to(device)\n    \n    optimizer_parameters = get_optimizer_params(model,\n                                                encoder_lr=CFG.ENCODER_LR, \n                                                decoder_lr=CFG.DECODER_LR,\n                                                weight_decay=CFG.WEIGHT_DECAY)\n    \n    optimizer = AdamW(optimizer_parameters, lr=CFG.ENCODER_LR, eps=CFG.EPS, betas=CFG.BETAS)\n    \n    num_train_batches = len(train_df)// CFG.BATCH_SIZE\n    num_val_batches = len(valid_df)// CFG.BATCH_SIZE\n    num_train_steps = num_train_batches * CFG.NUM_EPOCHS\n    \n    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n    \n    # ====================================================\n    # loop\n    # ====================================================\n    criterion = torch.nn.BCEWithLogitsLoss(reduction=\"mean\")\n    \n    best_loss = CFG.INFINITY\n    best_epoch = 0\n    \n    train_dataset = PatentClassification_TrainDataset(CFG, cpc_code_dict, train_df)\n    valid_dataset = PatentClassification_TrainDataset(CFG, cpc_code_dict, valid_df)\n    \n    #num_workers = 2 gives best speed (for GPU)\n    train_dataloader = DataLoader(train_dataset, batch_size = CFG.BATCH_SIZE, drop_last=True, num_workers = 2, collate_fn = custom_collate_fn) \n    valid_dataloader = DataLoader(valid_dataset, batch_size = CFG.BATCH_SIZE, drop_last=True, num_workers = 2, collate_fn = custom_collate_fn) \n    \n    \n    for epoch in range(CFG.NUM_EPOCHS):        \n        #train\n        avg_train_loss = train_fn(train_dataloader, num_train_batches, model, criterion, optimizer, epoch, scheduler, device)\n        \n        #eval\n        overall_val_loss = valid_fn(valid_dataloader, num_val_batches, model, criterion, device)\n        \n        elapsed = time.time() - start_time\n        \n        print(f\"Epoch {epoch+1} - avg_train_loss: {avg_train_loss:.4f}, overall_val_loss: {overall_val_loss: 4f} time: {elapsed:.0f}s\")\n        torch.save({'model': model.state_dict()}, f\"BERT_For_Patents_FineTuned_{epoch+1}.pth\")\n            \n        if overall_val_loss < best_loss:\n            best_loss = overall_val_loss\n            best_epoch = epoch + 1\n            torch.save({'model': model.state_dict()}, \"BERT_For_Patents_FineTuned.pth\")\n            \n    print(f'Best Epoch {best_epoch} - Best Loss: {best_loss:.4f}')\n            \n    torch.cuda.empty_cache()\n    gc.collect()\n              ","metadata":{"id":"tVEz812xC0_S","execution":{"iopub.status.busy":"2022-10-16T16:44:15.612880Z","iopub.execute_input":"2022-10-16T16:44:15.613234Z","iopub.status.idle":"2022-10-16T16:44:15.626913Z","shell.execute_reply.started":"2022-10-16T16:44:15.613198Z","shell.execute_reply":"2022-10-16T16:44:15.625835Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    \n    if CFG.F_TRAIN:\n        torch.cuda.empty_cache()\n        gc.collect()\n        train_loop()\n        ","metadata":{"id":"oKG0pRqtC0_T","outputId":"a0685932-b74e-4ecc-fa7f-f17b1aaa8df2","execution":{"iopub.status.busy":"2022-10-16T16:44:15.629587Z","iopub.execute_input":"2022-10-16T16:44:15.630163Z","iopub.status.idle":"2022-10-16T16:47:34.966282Z","shell.execute_reply.started":"2022-10-16T16:44:15.630127Z","shell.execute_reply":"2022-10-16T16:47:34.964802Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at ../input/bert-for-patents/bert-for-patents-pytorch were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Model run time:  1.7841768264770508  s\nEpoch: [1][0/56982] Elapsed 0m 1s (remain 1875m 47s) Loss: 0.7345(0.7345) Grad: 54025.5586  LR: 0.00002000  \nModel run time:  0.6901571750640869  s\nModel run time:  0.6913838386535645  s\nModel run time:  0.7221977710723877  s\nModel run time:  0.7151882648468018  s\nModel run time:  0.6887505054473877  s\nEpoch: [1][5/56982] Elapsed 0m 6s (remain 998m 35s) Loss: 0.6767(0.7030) Grad: 50488.7656  LR: 0.00002000  \nModel run time:  0.6894056797027588  s\nModel run time:  0.6896500587463379  s\nModel run time:  0.6899831295013428  s\nModel run time:  0.6896588802337646  s\nModel run time:  0.6884317398071289  s\nEpoch: [1][10/56982] Elapsed 0m 10s (remain 904m 30s) Loss: 0.6275(0.6774) Grad: 33841.3320  LR: 0.00002000  \nModel run time:  0.6894235610961914  s\nModel run time:  0.688934326171875  s\nModel run time:  0.6888992786407471  s\nModel run time:  0.6895186901092529  s\nModel run time:  0.6885337829589844  s\nEpoch: [1][15/56982] Elapsed 0m 14s (remain 869m 15s) Loss: 0.5786(0.6520) Grad: 28892.3145  LR: 0.00002000  \nModel run time:  0.6987893581390381  s\nModel run time:  0.6893868446350098  s\nModel run time:  0.6884622573852539  s\nModel run time:  0.6901288032531738  s\nModel run time:  0.6902778148651123  s\nEpoch: [1][20/56982] Elapsed 0m 18s (remain 851m 43s) Loss: 0.5347(0.6283) Grad: 27276.9746  LR: 0.00002000  \nModel run time:  0.6904895305633545  s\nModel run time:  0.6890931129455566  s\nModel run time:  0.6887404918670654  s\nModel run time:  0.6891653537750244  s\nModel run time:  0.68896484375  s\nEpoch: [1][25/56982] Elapsed 0m 23s (remain 840m 24s) Loss: 0.4978(0.6060) Grad: 27289.4531  LR: 0.00002000  \nModel run time:  0.6895818710327148  s\nModel run time:  0.6889302730560303  s\nModel run time:  0.6908667087554932  s\nModel run time:  0.6943032741546631  s\nModel run time:  0.6913051605224609  s\nEpoch: [1][30/56982] Elapsed 0m 27s (remain 833m 7s) Loss: 0.4610(0.5849) Grad: 25113.1230  LR: 0.00002000  \nModel run time:  0.6891875267028809  s\nModel run time:  0.68849778175354  s\nModel run time:  0.688727855682373  s\nModel run time:  0.6886842250823975  s\nModel run time:  0.688946008682251  s\nEpoch: [1][35/56982] Elapsed 0m 31s (remain 827m 15s) Loss: 0.4284(0.5649) Grad: 26667.7363  LR: 0.00002000  \nModel run time:  0.6900970935821533  s\nModel run time:  0.6903951168060303  s\nModel run time:  0.6895203590393066  s\nModel run time:  0.6891322135925293  s\nModel run time:  0.6895523071289062  s\nEpoch: [1][40/56982] Elapsed 0m 35s (remain 822m 49s) Loss: 0.3950(0.5457) Grad: 22228.0488  LR: 0.00002000  \nModel run time:  0.6934900283813477  s\nModel run time:  0.7175149917602539  s\nModel run time:  0.7011511325836182  s\nModel run time:  0.690192699432373  s\nModel run time:  0.6907901763916016  s\nEpoch: [1][45/56982] Elapsed 0m 39s (remain 822m 18s) Loss: 0.3649(0.5275) Grad: 21709.3359  LR: 0.00002000  \nModel run time:  0.6899933815002441  s\nModel run time:  0.6900298595428467  s\nModel run time:  0.6912901401519775  s\nModel run time:  0.6889967918395996  s\nModel run time:  0.6914327144622803  s\nEpoch: [1][50/56982] Elapsed 0m 44s (remain 819m 24s) Loss: 0.3376(0.5099) Grad: 20639.8340  LR: 0.00002000  \nModel run time:  0.6895360946655273  s\nModel run time:  0.6900198459625244  s\nModel run time:  0.6889395713806152  s\nModel run time:  0.6900031566619873  s\nModel run time:  0.6954653263092041  s\nEpoch: [1][55/56982] Elapsed 0m 48s (remain 816m 58s) Loss: 0.3102(0.4931) Grad: 19119.8711  LR: 0.00002000  \nModel run time:  0.690960168838501  s\nModel run time:  0.6896486282348633  s\nModel run time:  0.6904265880584717  s\nModel run time:  0.6886813640594482  s\nModel run time:  0.6898610591888428  s\nEpoch: [1][60/56982] Elapsed 0m 52s (remain 815m 1s) Loss: 0.2841(0.4768) Grad: 18870.7656  LR: 0.00002000  \nModel run time:  0.6900055408477783  s\nModel run time:  0.6896810531616211  s\nModel run time:  0.6900382041931152  s\nModel run time:  0.6895096302032471  s\nModel run time:  0.6905736923217773  s\nEpoch: [1][65/56982] Elapsed 0m 56s (remain 813m 12s) Loss: 0.2610(0.4611) Grad: 17827.6230  LR: 0.00002000  \nModel run time:  0.6891734600067139  s\nModel run time:  0.6896533966064453  s\nModel run time:  0.6977648735046387  s\nModel run time:  0.6904120445251465  s\nModel run time:  0.6895387172698975  s\nEpoch: [1][70/56982] Elapsed 1m 0s (remain 811m 52s) Loss: 0.2382(0.4460) Grad: 15692.3457  LR: 0.00002000  \nModel run time:  0.6900081634521484  s\nModel run time:  0.6903061866760254  s\nModel run time:  0.6895992755889893  s\nModel run time:  0.6892669200897217  s\nModel run time:  0.6979975700378418  s\nEpoch: [1][75/56982] Elapsed 1m 4s (remain 810m 40s) Loss: 0.2186(0.4315) Grad: 14839.4443  LR: 0.00001999  \nModel run time:  0.6896510124206543  s\nModel run time:  0.6891765594482422  s\nModel run time:  0.6894192695617676  s\nModel run time:  0.6952340602874756  s\nModel run time:  0.7006678581237793  s\nEpoch: [1][80/56982] Elapsed 1m 9s (remain 809m 52s) Loss: 0.1985(0.4176) Grad: 13687.4844  LR: 0.00001999  \nModel run time:  0.7032980918884277  s\nModel run time:  0.6895651817321777  s\nModel run time:  0.6899645328521729  s\nModel run time:  0.6902766227722168  s\nModel run time:  0.6890649795532227  s\nEpoch: [1][85/56982] Elapsed 1m 13s (remain 809m 6s) Loss: 0.1810(0.4043) Grad: 12628.2080  LR: 0.00001999  \nModel run time:  0.6899261474609375  s\nModel run time:  0.6887679100036621  s\nModel run time:  0.6902830600738525  s\nModel run time:  0.69687819480896  s\nModel run time:  0.6888835430145264  s\nEpoch: [1][90/56982] Elapsed 1m 17s (remain 808m 14s) Loss: 0.1647(0.3915) Grad: 11586.5410  LR: 0.00001999  \nModel run time:  0.6899571418762207  s\nModel run time:  0.689389705657959  s\nModel run time:  0.6895420551300049  s\nModel run time:  0.6898751258850098  s\nModel run time:  0.6901829242706299  s\nEpoch: [1][95/56982] Elapsed 1m 21s (remain 807m 25s) Loss: 0.1506(0.3793) Grad: 10789.8340  LR: 0.00001999  \nModel run time:  0.6895301342010498  s\nModel run time:  0.6891343593597412  s\nModel run time:  0.689288854598999  s\nModel run time:  0.6894111633300781  s\nModel run time:  0.6893258094787598  s\nEpoch: [1][100/56982] Elapsed 1m 25s (remain 806m 31s) Loss: 0.1381(0.3675) Grad: 10212.3174  LR: 0.00001999  \nModel run time:  0.690453290939331  s\nModel run time:  0.6889643669128418  s\nModel run time:  0.6892077922821045  s\nModel run time:  0.6890408992767334  s\nModel run time:  0.691107988357544  s\nEpoch: [1][105/56982] Elapsed 1m 30s (remain 805m 43s) Loss: 0.1240(0.3563) Grad: 9030.9141  LR: 0.00001999  \nModel run time:  0.6898131370544434  s\nModel run time:  0.6898608207702637  s\nModel run time:  0.6971821784973145  s\nModel run time:  0.6897716522216797  s\nModel run time:  0.691032886505127  s\nEpoch: [1][110/56982] Elapsed 1m 34s (remain 805m 4s) Loss: 0.1157(0.3456) Grad: 8360.7979  LR: 0.00001999  \nModel run time:  0.6892321109771729  s\nModel run time:  0.6910371780395508  s\nModel run time:  0.6899068355560303  s\nModel run time:  0.6888670921325684  s\nModel run time:  0.6890990734100342  s\nEpoch: [1][115/56982] Elapsed 1m 38s (remain 804m 22s) Loss: 0.1038(0.3354) Grad: 7440.1167  LR: 0.00001999  \nModel run time:  0.6897120475769043  s\nModel run time:  0.6968142986297607  s\nModel run time:  0.6912412643432617  s\nModel run time:  0.6893737316131592  s\nModel run time:  0.690587043762207  s\nEpoch: [1][120/56982] Elapsed 1m 42s (remain 803m 58s) Loss: 0.0945(0.3256) Grad: 6780.9155  LR: 0.00001999  \nModel run time:  0.6985766887664795  s\nModel run time:  0.6900031566619873  s\nModel run time:  0.6886882781982422  s\nModel run time:  0.6889574527740479  s\nModel run time:  0.6893439292907715  s\nEpoch: [1][125/56982] Elapsed 1m 46s (remain 803m 28s) Loss: 0.0871(0.3163) Grad: 6194.0894  LR: 0.00001999  \nModel run time:  0.6893582344055176  s\nModel run time:  0.689924955368042  s\nModel run time:  0.6888952255249023  s\nModel run time:  0.6895320415496826  s\nModel run time:  0.6889934539794922  s\nEpoch: [1][130/56982] Elapsed 1m 51s (remain 802m 53s) Loss: 0.0790(0.3074) Grad: 5657.0986  LR: 0.00001999  \nModel run time:  0.6232874393463135  s\nModel run time:  0.6914713382720947  s\nModel run time:  0.6898674964904785  s\nModel run time:  0.6933553218841553  s\nModel run time:  0.689424991607666  s\nEpoch: [1][135/56982] Elapsed 1m 55s (remain 802m 2s) Loss: 0.0739(0.2989) Grad: 2605.8596  LR: 0.00001999  \nModel run time:  0.6897640228271484  s\nModel run time:  0.6894795894622803  s\nModel run time:  0.688662052154541  s\nModel run time:  0.6901729106903076  s\nModel run time:  0.6891202926635742  s\nEpoch: [1][140/56982] Elapsed 1m 59s (remain 801m 33s) Loss: 0.0699(0.2908) Grad: 2254.8416  LR: 0.00001999  \nModel run time:  0.6892335414886475  s\nModel run time:  0.6889753341674805  s\nModel run time:  0.6900756359100342  s\nModel run time:  0.6901819705963135  s\nModel run time:  0.6896355152130127  s\nEpoch: [1][145/56982] Elapsed 2m 3s (remain 801m 6s) Loss: 0.0585(0.2829) Grad: 1997.8473  LR: 0.00001999  \nModel run time:  0.6899218559265137  s\nModel run time:  0.6994051933288574  s\nModel run time:  0.6905899047851562  s\nModel run time:  0.6898632049560547  s\nModel run time:  0.6891021728515625  s\nEpoch: [1][150/56982] Elapsed 2m 7s (remain 800m 49s) Loss: 0.0514(0.2754) Grad: 1689.4706  LR: 0.00001999  \nModel run time:  0.6891257762908936  s\nModel run time:  0.6918823719024658  s\nModel run time:  0.6891531944274902  s\nModel run time:  0.6889851093292236  s\nModel run time:  0.6951017379760742  s\nEpoch: [1][155/56982] Elapsed 2m 11s (remain 800m 26s) Loss: 0.0447(0.2681) Grad: 1481.8507  LR: 0.00001999  \nModel run time:  0.6915788650512695  s\nModel run time:  0.6894023418426514  s\nModel run time:  0.6900837421417236  s\nModel run time:  0.6913325786590576  s\nModel run time:  0.7002274990081787  s\nEpoch: [1][160/56982] Elapsed 2m 16s (remain 800m 12s) Loss: 0.0440(0.2612) Grad: 1270.6151  LR: 0.00001999  \nModel run time:  0.6892616748809814  s\nModel run time:  0.6891944408416748  s\nModel run time:  0.6891708374023438  s\nModel run time:  0.6910467147827148  s\nModel run time:  0.6899526119232178  s\nEpoch: [1][165/56982] Elapsed 2m 20s (remain 799m 57s) Loss: 0.0362(0.2545) Grad: 1131.9032  LR: 0.00001999  \nModel run time:  0.6895492076873779  s\nModel run time:  0.6893362998962402  s\nModel run time:  0.6889815330505371  s\nModel run time:  0.6901655197143555  s\nModel run time:  0.6902742385864258  s\nEpoch: [1][170/56982] Elapsed 2m 24s (remain 799m 35s) Loss: 0.0344(0.2481) Grad: 994.9158  LR: 0.00001999  \nModel run time:  0.6891653537750244  s\nModel run time:  0.6902191638946533  s\nModel run time:  0.6898093223571777  s\nModel run time:  0.6916458606719971  s\nModel run time:  0.6898980140686035  s\nEpoch: [1][175/56982] Elapsed 2m 28s (remain 799m 21s) Loss: 0.0340(0.2421) Grad: 918.0115  LR: 0.00001999  \nModel run time:  0.6907749176025391  s\nModel run time:  0.6927213668823242  s\nModel run time:  0.6891279220581055  s\nModel run time:  0.6898887157440186  s\nModel run time:  0.6891043186187744  s\nEpoch: [1][180/56982] Elapsed 2m 32s (remain 799m 3s) Loss: 0.0308(0.2362) Grad: 851.2130  LR: 0.00001999  \nModel run time:  0.689124584197998  s\nModel run time:  0.6896986961364746  s\nModel run time:  0.6899352073669434  s\nModel run time:  0.689302921295166  s\nModel run time:  0.6886487007141113  s\nEpoch: [1][185/56982] Elapsed 2m 36s (remain 798m 43s) Loss: 0.0290(0.2307) Grad: 815.2363  LR: 0.00001999  \nModel run time:  0.6892321109771729  s\nModel run time:  0.7051985263824463  s\nModel run time:  0.6906945705413818  s\nModel run time:  0.6899926662445068  s\nModel run time:  0.6913783550262451  s\nEpoch: [1][190/56982] Elapsed 2m 41s (remain 798m 34s) Loss: 0.0292(0.2254) Grad: 765.1339  LR: 0.00001999  \nModel run time:  0.6899054050445557  s\nModel run time:  0.6892764568328857  s\nModel run time:  0.6947407722473145  s\nModel run time:  0.689784049987793  s\nModel run time:  0.6902379989624023  s\nEpoch: [1][195/56982] Elapsed 2m 45s (remain 798m 30s) Loss: 0.0253(0.2203) Grad: 686.2930  LR: 0.00001999  \nModel run time:  0.6910617351531982  s\nModel run time:  0.6893062591552734  s\nModel run time:  0.6908047199249268  s\nModel run time:  0.6886634826660156  s\nModel run time:  0.6956436634063721  s\nEpoch: [1][200/56982] Elapsed 2m 49s (remain 798m 16s) Loss: 0.0245(0.2155) Grad: 688.3994  LR: 0.00001999  \nModel run time:  0.6902608871459961  s\nModel run time:  0.6895229816436768  s\nModel run time:  0.6903173923492432  s\nModel run time:  0.6903252601623535  s\nModel run time:  0.6896553039550781  s\nEpoch: [1][205/56982] Elapsed 2m 53s (remain 798m 1s) Loss: 0.0285(0.2109) Grad: 686.0057  LR: 0.00001999  \nModel run time:  0.6896448135375977  s\nModel run time:  0.6892907619476318  s\nModel run time:  0.688978910446167  s\nModel run time:  0.6892495155334473  s\nModel run time:  0.6904051303863525  s\nEpoch: [1][210/56982] Elapsed 2m 57s (remain 797m 44s) Loss: 0.0280(0.2065) Grad: 751.4127  LR: 0.00001999  \nModel run time:  0.6908600330352783  s\nModel run time:  0.6889870166778564  s\nModel run time:  0.6934678554534912  s\nModel run time:  0.6933505535125732  s\nModel run time:  0.6889121532440186  s\nEpoch: [1][215/56982] Elapsed 3m 2s (remain 797m 34s) Loss: 0.0280(0.2023) Grad: 724.4989  LR: 0.00001998  \nModel run time:  0.6890912055969238  s\nModel run time:  0.6893255710601807  s\nModel run time:  0.6894943714141846  s\nModel run time:  0.6914138793945312  s\nModel run time:  0.6893713474273682  s\nEpoch: [1][220/56982] Elapsed 3m 6s (remain 797m 19s) Loss: 0.0278(0.1983) Grad: 656.8798  LR: 0.00001998  \nModel run time:  0.6898527145385742  s\nModel run time:  0.6889932155609131  s\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/354130615.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_17/3406122111.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_train_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m#eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_17/1638509045.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(train_dataloader, num_batches, model, criterion, optimizer, epoch, scheduler, device)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# Calculate and scale gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mgrad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAX_GRAD_NORM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# Update Weights and Biases, Scaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror_if_nonfinite\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_or\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         raise RuntimeError(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror_if_nonfinite\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_or\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         raise RuntimeError(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m   1544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0m_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# noqa: C416 TODO: rewrite as list(range(m))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1546\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1548\u001b[0m     \u001b[0;31m# TODO: when https://github.com/pytorch/pytorch/issues/33782 is fixed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}